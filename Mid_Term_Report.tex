\documentclass[12pt,a4paper]{article}
\usepackage[utf8]{inputenc}
\usepackage[margin=1in]{geometry}
\usepackage{amsmath}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{graphicx}
\usepackage{hyperref}
\usepackage{booktabs}
\usepackage{listings}
\usepackage{xcolor}
\usepackage{float}
\usepackage{subcaption}
\usepackage{algorithm}
\usepackage{algorithmic}
\usepackage{cite}
\usepackage{fancyhdr}

% Code listing style
\lstset{
    backgroundcolor=\color{gray!10},
    basicstyle=\ttfamily\small,
    breaklines=true,
    frame=single,
    numbers=left,
    numberstyle=\tiny,
    keywordstyle=\color{blue},
    commentstyle=\color{green!60!black},
    stringstyle=\color{red}
}

% Header and footer
\pagestyle{fancy}
\fancyhf{}
\rhead{PPO-Based UAV Navigation}
\lhead{Mid-Term Report}
\cfoot{\thepage}

\title{\textbf{Autonomous UAV Navigation using Proximal Policy Optimization\\
with LIDAR-based Obstacle Avoidance}}
\author{Student Name\\
Department of Computer Science/Engineering\\
University Name}
\date{\today}

\begin{document}

\maketitle
\thispagestyle{empty}

\newpage

% Abstract
\section{Abstract}

This report presents the mid-term progress of developing an autonomous UAV navigation system using Proximal Policy Optimization (PPO) reinforcement learning algorithm. The project focuses on training a quadrotor UAV to navigate from a predefined start position to a goal position while avoiding static obstacles in a 3D environment. The system incorporates a physics-based simulation using MuJoCo, enhanced with LIDAR sensor integration for 360-degree environmental perception and boundary enforcement mechanisms. The PPO agent utilizes a 25-dimensional observation space including position, velocity, goal distance vectors, and 16-ray LIDAR readings to make informed navigation decisions. Initial results demonstrate significant improvement in navigation performance, with average rewards increasing from 0-1 range to 2-4 range after implementing optimized hyperparameters and adaptive action standard deviation decay. The system successfully handles collision detection, boundary violations, and goal reaching scenarios with appropriate reward structures. Current achievements include stable training convergence, effective obstacle avoidance using LIDAR data, and robust boundary enforcement with -10 penalty for out-of-bounds violations.

\textbf{Keywords:} Reinforcement Learning, PPO, UAV Navigation, LIDAR, Obstacle Avoidance, MuJoCo Simulation

\newpage

% Table of Contents
\tableofcontents
\newpage

% Introduction
\section{Introduction}

\subsection{Background}
Autonomous Unmanned Aerial Vehicles (UAVs) have gained tremendous importance in various applications including search and rescue operations, surveillance, package delivery, and environmental monitoring. The ability to navigate autonomously in complex environments while avoiding obstacles is crucial for the practical deployment of UAVs in real-world scenarios.

Traditional path planning algorithms often rely on pre-computed maps and predetermined waypoints, which may not be suitable for dynamic environments or unknown terrains. Reinforcement Learning (RL) offers a promising alternative by enabling UAVs to learn optimal navigation policies through interaction with the environment.

\subsection{Problem Statement}
The primary challenge addressed in this project is to develop an autonomous navigation system for a quadrotor UAV that can:
\begin{itemize}
    \item Navigate from a static start position to a predefined goal position
    \item Avoid static obstacles of varying shapes and sizes
    \item Maintain flight within designated boundaries
    \item Utilize sensor data for environmental perception
    \item Learn optimal policies through reinforcement learning
\end{itemize}

\subsection{Objectives}
\begin{enumerate}
    \item Implement a physics-based UAV simulation environment using MuJoCo
    \item Develop a PPO-based reinforcement learning agent for navigation control
    \item Integrate LIDAR sensor system for obstacle detection and avoidance
    \item Design an effective reward system for training convergence
    \item Evaluate the system performance through comprehensive testing
\end{enumerate}

\subsection{Scope}
This project focuses on 3D navigation in static environments with known start and goal positions. The scope includes:
\begin{itemize}
    \item Single UAV navigation (no multi-agent scenarios)
    \item Static obstacles (no dynamic obstacle movement)
    \item Perfect sensor readings (no noise modeling)
    \item Discrete training episodes with environment resets
\end{itemize}

% Specification and Design
\section{Specification and Design}

\subsection{System Architecture}
The system comprises four main components as illustrated in Figure \ref{fig:architecture}:

\begin{enumerate}
    \item \textbf{MuJoCo Simulation Environment}: Physics-based 3D simulation
    \item \textbf{PPO Agent}: Deep reinforcement learning controller
    \item \textbf{LIDAR Sensor System}: 360-degree obstacle detection
    \item \textbf{Reward System}: Training signal generation
\end{enumerate}

\subsection{Environment Specifications}
\begin{table}[H]
\centering
\caption{Environment Configuration Parameters}
\begin{tabular}{@{}ll@{}}
\toprule
Parameter & Value \\
\midrule
World Size & 8.0 × 8.0 × 5.0 meters \\
Start Position & (-4.0, -4.0, 1.8) \\
Goal Position & (4.0, 4.0, 1.8) \\
UAV Flight Height & 1.8 meters \\
Static Obstacles & 8 randomly placed \\
Obstacle Height & 2.0 meters \\
Collision Distance & 0.2 meters \\
Max Episode Steps & 50,000 \\
\bottomrule
\end{tabular}
\end{table}

\subsection{UAV Model Design}
The quadrotor UAV model includes:
\begin{itemize}
    \item Main chassis: 0.20 × 0.20 × 0.04 meters (red colored)
    \item Four motors with propellers at corners
    \item Four actuation sites for thrust control
    \item Mass: 0.8 kg
    \item Action space: 4 continuous thrust values [1.0, 10.0]
\end{itemize}

\subsection{LIDAR Sensor Integration}
The LIDAR system specifications:
\begin{itemize}
    \item 360-degree coverage with 16 rays
    \item Maximum detection range: 3.0 meters
    \item Ray casting for obstacle and boundary detection
    \item Real-time distance measurements
\end{itemize}

\subsection{Observation Space Design}
The observation vector contains 25 dimensions:
\begin{align}
\mathbf{s}_t = [\mathbf{p}_t, \mathbf{v}_t, \mathbf{d}_{goal}, \mathbf{L}_t]
\end{align}

Where:
\begin{itemize}
    \item $\mathbf{p}_t \in \mathbb{R}^3$: Current position (x, y, z)
    \item $\mathbf{v}_t \in \mathbb{R}^3$: Current velocity (vx, vy, vz)
    \item $\mathbf{d}_{goal} \in \mathbb{R}^3$: Distance vector to goal
    \item $\mathbf{L}_t \in \mathbb{R}^{16}$: LIDAR distance readings
\end{itemize}

\subsection{Reward Function Design}
The reward function $R(s_t, a_t, s_{t+1})$ includes:

\begin{align}
R(s_t, a_t, s_{t+1}) = \begin{cases}
+100 & \text{if goal reached} \\
-100 & \text{if obstacle collision} \\
-10 & \text{if boundary violation} \\
+0.1 & \text{if moving toward goal} \\
0 & \text{otherwise}
\end{cases}
\end{align}

\subsection{PPO Algorithm Implementation}
The PPO agent utilizes:
\begin{itemize}
    \item Actor-Critic architecture with 64-neuron hidden layers
    \item Gaussian policy with learnable standard deviation
    \item Clipped surrogate objective for policy updates
    \item Value function approximation for advantage estimation
\end{itemize}

\begin{algorithm}
\caption{PPO Training Algorithm}
\begin{algorithmic}
\FOR{episode = 1 to max\_episodes}
    \STATE Initialize environment and get initial state $s_0$
    \FOR{step = 1 to max\_steps}
        \STATE Select action $a_t \sim \pi_\theta(a_t|s_t)$
        \STATE Execute action and observe $s_{t+1}, r_t$
        \STATE Store $(s_t, a_t, r_t, \log\pi_\theta(a_t|s_t))$
        \IF{update\_timestep reached}
            \STATE Compute advantages and returns
            \STATE Update policy using PPO objective
            \STATE Clear memory buffer
        \ENDIF
    \ENDFOR
\ENDFOR
\end{algorithmic}
\end{algorithm}

% Progress and Results
\section{Progress Till Date Along with Results}

\subsection{Implementation Milestones}
\begin{enumerate}
    \item \textbf{Environment Setup}: Successfully implemented MuJoCo-based UAV simulation
    \item \textbf{PPO Agent}: Developed complete actor-critic architecture
    \item \textbf{LIDAR Integration}: Added 16-ray LIDAR sensor system
    \item \textbf{Boundary Enforcement}: Implemented boundary violation detection
    \item \textbf{Training Optimization}: Optimized hyperparameters for improved performance
\end{enumerate}

\subsection{Training Performance Analysis}

\begin{table}[H]
\centering
\caption{Training Performance Comparison}
\begin{tabular}{@{}lcc@{}}
\toprule
Metric & Before Optimization & After Optimization \\
\midrule
Average Reward Range & 0 - 1 & 2 - 4 \\
Average Episode Length & 99 - 151 steps & 130 - 151 steps \\
Training Stability & Fluctuating & More Consistent \\
Convergence Speed & Slow & Improved \\
Best Recorded Reward & 2.0 & 4.3 \\
\bottomrule
\end{tabular}
\end{table}

\subsection{Hyperparameter Optimization Results}

The following optimizations significantly improved training performance:

\begin{table}[H]
\centering
\caption{Optimized Hyperparameters}
\begin{tabular}{@{}lcc@{}}
\toprule
Parameter & Original Value & Optimized Value \\
\midrule
Action Standard Deviation & 0.6 & 0.3 \\
Update Timestep & 4000 & 2048 \\
PPO Epochs & 80 & 10 \\
Epsilon Clip & 0.2 & 0.1 \\
Actor Learning Rate & 3×10⁻⁴ & 1×10⁻⁴ \\
Critic Learning Rate & 1×10⁻³ & 5×10⁻⁴ \\
\bottomrule
\end{tabular}
\end{table}

\subsection{Key Achievements}

\subsubsection{LIDAR-based Navigation}
The integration of 16-ray LIDAR system enables:
\begin{itemize}
    \item 360-degree environmental awareness
    \item Precise obstacle distance measurements
    \item Improved collision avoidance capabilities
    \item Enhanced spatial reasoning for the agent
\end{itemize}

\subsubsection{Boundary Enforcement}
Successfully implemented boundary checking with:
\begin{itemize}
    \item Real-time position monitoring
    \item Immediate episode termination for violations
    \item -10 reward penalty for out-of-bounds flight
    \item Effective training signal for boundary awareness
\end{itemize}

\subsubsection{Training Convergence}
Achieved stable training with:
\begin{itemize}
    \item Consistent reward improvements
    \item Reduced training variance
    \item Faster policy updates
    \item Better exploration-exploitation balance
\end{itemize}

\subsection{Performance Metrics}

Recent training episodes (4060-4155) showed:
\begin{itemize}
    \item Average reward improvement: 0.4 → 3.9
    \item Consistent positive rewards in most episodes
    \item Successful goal-reaching behaviors
    \item Effective obstacle avoidance strategies
\end{itemize}

\subsection{Visualization Results}
The rendering system demonstrates:
\begin{itemize}
    \item Clear UAV visibility with bright red main body
    \item Real-time path trail visualization
    \item Obstacle and boundary markers
    \item Collision detection feedback
\end{itemize}

\subsection{Code Structure and Organization}
\begin{itemize}
    \item \texttt{uav\_env.py}: Custom Gymnasium environment (242 lines)
    \item \texttt{ppo\_agent.py}: PPO algorithm implementation (145 lines)
    \item \texttt{training.py}: Training loop and optimization (155 lines)
    \item \texttt{uav\_render.py}: Visualization and testing (402 lines)
    \item \texttt{test\_lidar.py}: LIDAR functionality validation
\end{itemize}

\subsection{Testing and Validation}
Comprehensive testing included:
\begin{itemize}
    \item LIDAR sensor accuracy verification
    \item Boundary detection validation
    \item Collision system testing
    \item Reward function correctness
    \item Model loading/saving functionality
\end{itemize}

% Future Work Plan
\section{Future Work Plan}

\subsection{Short-term Goals (Next 4-6 weeks)}

\subsubsection{Performance Enhancement}
\begin{enumerate}
    \item \textbf{Network Architecture Improvement}
        \begin{itemize}
            \item Increase hidden layer sizes to 128/256 neurons
            \item Add third hidden layer for complex behavior modeling
            \item Implement LayerNorm for training stability
        \end{itemize}
    
    \item \textbf{Advanced Reward Engineering}
        \begin{itemize}
            \item Add smooth flight rewards (penalize rapid direction changes)
            \item Implement distance-based obstacle avoidance rewards
            \item Include energy efficiency considerations
        \end{itemize}
    
    \item \textbf{Training Algorithm Enhancements}
        \begin{itemize}
            \item Implement Generalized Advantage Estimation (GAE)
            \item Add experience replay buffer
            \item Implement curriculum learning with progressive difficulty
        \end{itemize}
\end{enumerate}

\subsubsection{Robustness Testing}
\begin{enumerate}
    \item Extensive evaluation with multiple obstacle configurations
    \item Statistical analysis of success rates over 1000+ episodes
    \item Performance benchmarking against baseline algorithms
    \item Stress testing with varying environment parameters
\end{enumerate}

\subsection{Medium-term Goals (6-10 weeks)}

\subsubsection{Advanced Features}
\begin{enumerate}
    \item \textbf{Dynamic Obstacles}
        \begin{itemize}
            \item Moving obstacle integration
            \item Predictive collision avoidance
            \item Adaptive LIDAR scanning strategies
        \end{itemize}
    
    \item \textbf{Multi-goal Navigation}
        \begin{itemize}
            \item Sequential waypoint navigation
            \item Optimal path planning integration
            \item Goal prioritization mechanisms
        \end{itemize}
    
    \item \textbf{Sensor Noise Modeling}
        \begin{itemize}
            \item Realistic LIDAR noise simulation
            \item GPS uncertainty modeling
            \item Robust estimation techniques
        \end{itemize}
\end{enumerate}

\subsubsection{Real-world Preparation}
\begin{enumerate}
    \item Hardware-in-the-loop simulation setup
    \item Real UAV model parameter tuning
    \item Communication protocol development
    \item Safety system implementation
\end{enumerate}

\subsection{Long-term Vision (10+ weeks)}

\subsubsection{Advanced Applications}
\begin{enumerate}
    \item Multi-agent UAV coordination
    \item Emergency response scenarios
    \item Search and rescue mission simulation
    \item Package delivery optimization
\end{enumerate}

\subsubsection{Research Contributions}
\begin{enumerate}
    \item Comparative study with other RL algorithms
    \item Novel reward function designs
    \item LIDAR-based navigation techniques
    \item Real-world deployment case studies
\end{enumerate}

\subsection{Deliverables Timeline}

\begin{table}[H]
\centering
\caption{Project Timeline and Deliverables}
\begin{tabular}{@{}llp{6cm}@{}}
\toprule
Timeline & Milestone & Deliverables \\
\midrule
Week 1-2 & Performance Optimization & Enhanced network architecture, improved training algorithms \\
Week 3-4 & Robustness Testing & Comprehensive evaluation report, statistical analysis \\
Week 5-6 & Advanced Features & Dynamic obstacles, multi-goal navigation \\
Week 7-8 & Real-world Prep & Hardware integration, safety systems \\
Week 9-10 & Final Integration & Complete system testing, documentation \\
Week 11-12 & Documentation & Final report, presentation materials \\
\bottomrule
\end{tabular}
\end{table}

\subsection{Risk Assessment and Mitigation}

\subsubsection{Technical Risks}
\begin{enumerate}
    \item \textbf{Training Instability}: Mitigation through careful hyperparameter tuning
    \item \textbf{Hardware Limitations}: Cloud computing resources as backup
    \item \textbf{Simulation-Reality Gap}: Gradual complexity increase, domain randomization
\end{enumerate}

\subsubsection{Project Risks}
\begin{enumerate}
    \item \textbf{Timeline Delays}: Buffer time allocation, priority-based development
    \item \textbf{Scope Creep}: Clear milestone definitions, regular progress reviews
\end{enumerate}

% References
\section{References}

\begin{thebibliography}{20}

\bibitem{schulman2017ppo}
Schulman, J., Wolski, F., Dhariwal, P., Radford, A., \& Klimov, O. (2017). 
\textit{Proximal policy optimization algorithms}. 
arXiv preprint arXiv:1707.06347.

\bibitem{brockman2016gym}
Brockman, G., Cheung, V., Pettersson, L., Schneider, J., Schulman, J., Tang, J., \& Zaremba, W. (2016). 
\textit{OpenAI gym}. 
arXiv preprint arXiv:1606.01540.

\bibitem{todorov2012mujoco}
Todorov, E., Erez, T., \& Tassa, Y. (2012). 
\textit{Mujoco: A physics engine for model-based control}. 
In 2012 IEEE/RSJ international conference on intelligent robots and systems (pp. 5026-5033). IEEE.

\bibitem{kober2013reinforcement}
Kober, J., Bagnell, J. A., \& Peters, J. (2013). 
\textit{Reinforcement learning in robotics: A survey}. 
The International Journal of Robotics Research, 32(11), 1238-1274.

\bibitem{tai2017virtual}
Tai, L., Paolo, G., \& Liu, M. (2017). 
\textit{Virtual-to-real deep reinforcement learning: Continuous control of mobile robots for mapless navigation}. 
In 2017 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS) (pp. 31-36). IEEE.

\bibitem{zhu2017target}
Zhu, Y., Mottaghi, R., Kolve, E., Lim, J. J., Gupta, A., Fei-Fei, L., \& Farhadi, A. (2017). 
\textit{Target-driven visual navigation in indoor scenes using deep reinforcement learning}. 
In 2017 IEEE international conference on robotics and automation (ICRA) (pp. 3357-3364). IEEE.

\bibitem{sutton2018reinforcement}
Sutton, R. S., \& Barto, A. G. (2018). 
\textit{Reinforcement learning: An introduction}. 
MIT press.

\bibitem{lillicrap2015continuous}
Lillicrap, T. P., Hunt, J. J., Pritzel, A., Heess, N., Erez, T., Tassa, Y., ... \& Wierstra, D. (2015). 
\textit{Continuous control with deep reinforcement learning}. 
arXiv preprint arXiv:1509.02971.

\bibitem{mnih2015human}
Mnih, V., Kavukcuoglu, K., Silver, D., Rusu, A. A., Veness, J., Bellemare, M. G., ... \& Hassabis, D. (2015). 
\textit{Human-level control through deep reinforcement learning}. 
nature, 518(7540), 529-533.

\bibitem{peng2018sim}
Peng, X. B., Andrychowicz, M., Zaremba, W., \& Abbeel, P. (2018). 
\textit{Sim-to-real transfer of robotic control with dynamics randomization}. 
In 2018 IEEE international conference on robotics and automation (ICRA) (pp. 3803-3810). IEEE.

\bibitem{zhu2018reinforcement}
Zhu, P., Zhou, L., \& Liu, X. (2018). 
\textit{Reinforcement learning based approach for UAV autonomous navigation and collision avoidance}. 
In 2018 IEEE International Conference on Robotics and Biomimetics (ROBIO) (pp. 1658-1664). IEEE.

\bibitem{kaelbling1996reinforcement}
Kaelbling, L. P., Littman, M. L., \& Moore, A. W. (1996). 
\textit{Reinforcement learning: A survey}. 
Journal of artificial intelligence research, 4, 237-285.

\end{thebibliography}

\end{document}